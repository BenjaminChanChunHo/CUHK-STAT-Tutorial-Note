---
title: "CUHK RMSC4002 Tutorial 6"
author: "Benjamin Chan"
date: "October 30, 2018"
output:
  html_document:
    keep_md: yes
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
```

### Reference
Textbook:
Ch8 Applied Multivariate Statistical Analysis (Johnson & Wichern)

(Optional) Series of Lecture Videos: </br>
1. Background Knowledge: Singular Value Decomposition (SVD) </br>
https://www.youtube.com/watch?v=EokL7E6o1AE&index=3&list=PLEQ-ymviqWk_KbnJKZo2-5ADUS748Uo18&t=10s </br>
2. Main Theory: Principal Componenet Analysis (PCA) </br>
https://www.youtube.com/watch?v=a9jdQGybYmE&list=PLEQ-ymviqWk_KbnJKZo2-5ADUS748Uo18&index=1 </br>
3. Application: Face Recognition </br>
https://www.youtube.com/watch?v=8BTv-KZ2Bh8&index=3&list=PLEQ-ymviqWk_KbnJKZo2-5ADUS748Uo18

(Optional) Linear Algebra Course by Professor Gilbert Strang at MIT: </br>
https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/

(Optional) Coursera - Mathematics for Machine Learning Specialization (Linear Algebra, Multivariate Calculus and PCA): </br>
https://www.coursera.org/specializations/mathematics-machine-learning

### Principal Component Analysis (PCA)
Principal Component Analysis is a traditional multivariate statistical technique for dimension or variable reduction. It aims to find linear combinations of original variables such that the information in the original data is preserved.

#### Theory
Setting: Let the random vector $X'=[X_1,X_2,\dots,X_p]$ have the covariance matrix $\Sigma$ with eigenvalues $\lambda_1\geq\lambda_2\geq\cdots\geq 0$. Consider the linear combinations
$$Y=\begin{bmatrix}Y_1\\ Y_2 \\ \vdots\\Y_p\end{bmatrix}=\begin{bmatrix}a_1'X\\ a_2'X \\ \vdots\\a_p'X\end{bmatrix}=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1p} \\ a_{21} & a_{22} & \cdots & a_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ a_{p1} & a_{p2} & \cdots & a_{pp}\end{bmatrix}\begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_p\end{bmatrix}=AX.$$
Then for $i,j=1,2,\dots,p$, $$\text{Var}(Y_i)=\text{Var}(a_i'X)=a_i'\Sigma a_i$$
$$\text{Cov}(Y_i,Y_j)=\text{Cov}(a_i'X,a_j'X)=a_i'\Sigma a_j$$
Objective: The principal components are those uncorrelated linear combinations $Y_1,Y_2,\dots,Y_p$ whose variances are as large as possible.

Algorithm: The first principal component is the linear combination $a_1'X$ that maximizes $\text{Var}(a_1'X)$ subject to $a_1'a_1=1$. At the $i\geq 2$ th step, the $i$th principal component is the linear combination $a_i'X$ that maximizes $\text{Var}(a_i'X)$ subject to $a_i'a_i=1$ and $\text{Cov}(a_i'X,a_j'X)=0$ for $j<i$.

Result 8.1 in the textbook (you can find the proof there): </br>
Let $\Sigma$ be the covariance matrix associated with the random vector $X'=[X_1,X_2,\dots,X_p]$. Let $\Sigma$ have the eigenvalue-eigenvector pairs $(\lambda_1,e_1), (\lambda_2,e_2),\dots,(\lambda_p,e_p)$ where $\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_p\geq0$. Then the $i$th principal component is given by 
$$Y_i=e_i'X=e_{i1}X_1+e_{i2}X_2+\cdots+e_{ip}X_p,\hspace{5mm} i=1,2,\dots,p$$ with 
$$\text{Var}(Y_i)=e_i'\Sigma e_i=\lambda_i,\hspace{5mm} i=1,2,\dots,p$$ and
$$\text{Cov}(Y_i,Y_j)=\text{Cov}(e_i'X,e_j'X)=e_i'\Sigma e_j=0 ,\hspace{5mm} i\neq j.$$
If some $\lambda_i$ are equal, the choices of the corresponding coefficient vectors, $e_i$, and hence $Y_i$, are not unique.

#### Read in and Manipulate Data
The file `us-rate.csv` contains US semi-annualized zero-coupon rates, measured in basis points (b.p.), with maturities between 1-month to 15-year, monthly data from 1944 to 1992.
```{r}
# Read in data (a CSV file) under Dataset
d <- read.csv("./../Dataset/us-rate.csv")
names(d)
label <- c("1m", "3m", "6m", "9m", "12m", "18", "2y", "3y", "4y", "5y", "7y", "10y", "15y")
names(d) <- label                # Apply labels
str(d)                           # Display the structure of an object

options(digits = 2)              # Display the number using 2 digits
cor(d)                           # Compute correlation matrix
```

Note that the 13 variables are highly correlated. The aim of dimension reduction is to use a few variables (say 2 or 3) to represent the data without loss of much information (maximizing variance). Each new variable comes from a linear combination of the original variables (see Theory).

#### Implementation of PCA
```{r}
options(digits = 4)              # Display the number using 4 digits
pca <- princomp(d, cor = T)      # Perform PCA using correlation matrix
pca$loadings[, 1:6]              # Display the loadings of the first six PC's

pc1 <- pca$loadings[, 1]         # Save the loading of 1st PC
pc2 <- pca$loadings[, 2]         # Save the loading of 2nd PC
pc3 <- pca$loadings[, 3]         # Save the loading of 3rd PC

par(mfrow = c(3, 1))             # Multiframe for plotting
plot(pc1, ylim = c(-0.6, 0.6), type = "o")
plot(pc2, ylim = c(-0.6, 0.6), type = "o") 
plot(pc3, ylim = c(-0.6, 0.6), type = "o")
```

Please refer to the lectures for intuitive interpretation of the loadings of the first three PCs. The 1st PC represents a parallel shift of the yield curve, the 2nd PC agrees with the liquidity preference theory while the 3rd PC represents the curvature of the yield curve.

```{r}
pc1 <- pca$loadings[, 1]         # Save the loading of 1st PC
pc2 <- pca$loadings[, 2]         # Save the loading of 2nd PC
pc3 <- pca$loadings[, 3]         # Save the loading of 3rd PC
pc1 %*% pc1                      # Should be 1 (unit length)
pc2 %*% pc2                      # Should be 1 (unit length)
pc1 %*% pc2                      # Should be 0 (orthogonal)

(s <- pca$sdev)                  # Save the s.d. of all PC's to s
round(s^2, 4)                    # Display the variances of all PC's
(t <- sum(s^2))                  # Compute total variance (should equal 13)
round(s^2/t, 4)                  # Proportion of variance explained by each PC
cumsum(s^2/t)                    # Cumulative sum of proportion of variance
```

By Lemma 1.10 in Tutorial 1 Proof: </br>
Let $R$ be a $p\times p$ symmetric matrix. Then, the trace of $R$ is $$\text{tr}(R)=\sum_{i=1}^{p}\lambda_i,$$ 
where the $\lambda_i$ are the eigenvalues of $R$.

The total variation is equal to the trace (sum of all the diagonal elements). Since the correlation matrix has been used, all the diagonal elements are 1 and the trace is equal to `r t`. The 1st PC has already explained `r as.vector(round(s^2/t, 4))[1]*100`% of the total variation (most information).

#### Scree Plot
```{r}
screeplot(pca, type = "lines")   # screeplot: plot variances against number of principal component
```

The plot supports that the 1st PC is sufficient to explain most information.